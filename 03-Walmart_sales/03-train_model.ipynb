{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart Sales: Basic linear regressor and regularization\n",
    "\n",
    "In this notebook I trained a first linear regressor on the Wallmart Sales data[(Kaggle competition)](https://www.kaggle.com/competitions/walmart-sales-forecasting/overview) to predict weekly sales using multiple variables.  \n",
    " \n",
    " In more detail, I:  \n",
    "- Pre-processed train and test sets before modeling:  \n",
    "    - **Imputed** certain **missing** explanatory **variables**\n",
    "    - **Scaled** any numerical explanatory variables and **encoded** categorical variables  \n",
    "- Applied a first **multivariate linear regressor** using:  \n",
    "    - Basic explanatory variables  \n",
    "    - Feature engineered variables  \n",
    "- Evaluated **model performance** throught a **cross validation**  \n",
    "- **Optimized** my linear **model** via **regularization** and **grid search** for hypeparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. Train and test set split\n",
    "2. Process variables: impute missing values / scale / onehot encode\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from feature_engine.imputation import RandomSampleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable length: 131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1572117.54', '1807545.43', '1244390.03', '1644470.66', '1857533.7']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data/processed/Walmart_Store_sales-targetvar.csv'\n",
    "with open(filename) as file:\n",
    "    Y = [line.rstrip() for line in file]\n",
    "\n",
    "print('Target variable length:',len(Y))\n",
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic explanatory variables shape: (131, 7)\n",
      "Basic explanatory variables: ['quarter', 'year', 'weekofyear', 'Holiday_Flag', 'Temperature', 'CPI', 'Unemployment']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0, 'y2', 7.0, nan, 15.33888888888889, 214.7775231, 6.858],\n",
       "       [1.0, 'y2', 12.0, 0.0, 5.766666666666668, 128.6160645, 7.47],\n",
       "       [nan, nan, nan, 0.0, 29.205555555555552, 214.55649680000005,\n",
       "        7.346]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_basic = pd.read_csv('data/processed/Walmart_Store_sales-expvar-basic.csv')\n",
    "print('Basic explanatory variables shape:', X_basic.shape)\n",
    "\n",
    "basic_vars_ls = X_basic.columns.tolist()\n",
    "print('Basic explanatory variables:', basic_vars_ls)\n",
    "\n",
    "X_basic = X_basic.values\n",
    "\n",
    "X_basic[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered explanatory variables shape: (131, 6)\n",
      "Engineered explanatory variables: ['quarter_str', 'year', 'Temperature_group', 'Store_group_CPI', 'Store_group_unemp', 'weekofyear_holiday']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['q1', 'y2', 'mean_temp', 'highsales_highCPI',\n",
       "        'highsales_lowunemp', nan],\n",
       "       ['q1', 'y2', 'low_temp', 'highsales_lowCPI',\n",
       "        'highsales_highunemp', 0.0],\n",
       "       [nan, nan, 'high_temp', 'lowsales_highCPI', 'lowsales_lowunemp',\n",
       "        0.0]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eng = pd.read_csv('data/processed/Walmart_Store_sales-expvar-feateng.csv')\n",
    "print('Engineered explanatory variables shape:', X_eng.shape)\n",
    "\n",
    "# Map certain categorical values to numerical values for missing value imputation\n",
    "\n",
    "\n",
    "eng_vars_ls = X_eng.columns.tolist()\n",
    "print('Engineered explanatory variables:', eng_vars_ls)\n",
    "\n",
    "X_eng = X_eng.values\n",
    "X_eng[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train and test set split  \n",
    "Choose a slightly smaller test size due to low number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_basic_train shape: (111, 7)\n",
      "X_basic_test shape: (20, 7)\n",
      "X_eng_train shape: (111, 6)\n",
      "X_eng_test shape: (20, 6)\n"
     ]
    }
   ],
   "source": [
    "X_basic_train, X_basic_test, Y_train, Y_test = train_test_split(X_basic, Y, test_size=0.15, random_state=0)\n",
    "\n",
    "X_eng_train, X_eng_test, Y_train, Y_test = train_test_split(X_eng, Y, test_size=0.15, random_state=0)\n",
    "\n",
    "print('X_basic_train shape:', X_basic_train.shape)\n",
    "print('X_basic_test shape:', X_basic_test.shape)\n",
    "\n",
    "print('X_eng_train shape:', X_eng_train.shape)\n",
    "print('X_eng_test shape:', X_eng_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process variables:  \n",
    "Impute missing values / scale / onehot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quarter',\n",
       " 'year',\n",
       " 'weekofyear',\n",
       " 'Holiday_Flag',\n",
       " 'Temperature',\n",
       " 'CPI',\n",
       " 'Unemployment']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic variables\n",
    "basic_vars_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quarter_str',\n",
       " 'year',\n",
       " 'Temperature_group',\n",
       " 'Store_group_CPI',\n",
       " 'Store_group_unemp',\n",
       " 'weekofyear_holiday']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Engineered variables\n",
    "eng_vars_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines for missing value imputations / scaling and one hot encoding\n",
    "\n",
    "# Categorical year\n",
    "# Impute less frequent: from EDA, year with least entries is 2010 (2010 has almost 52 weeks, 2011 37 weeks) \n",
    "# One hot encode\n",
    "year_feat = [1]\n",
    "year_transformer = Pipeline(\n",
    "    steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='y2')), \n",
    "    ('encoder', OneHotEncoder(drop='first')) # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "\n",
    "# Basic numerical variables: Holiday_Flag, Temperature, Unemployment\n",
    "# Based on distibution of data, use median for imputation\n",
    "basic_num_feats = [3, 4, 6] \n",
    "basic_num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Basic variables with more complex distribution: quarter, weekofyear and CPI (binomial distribution)\n",
    "# Multivariate imputation with Bayesian ridge: include already imputed temperature to aide imputation of other variables\n",
    "basic_multivar_feats = [0, 2, 5, 4]\n",
    "basic_multivar_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(max_iter=100, random_state=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Engineered categorical variables: Temperature_group, Store_group_CPI, Store_group_unemp\n",
    "# Impute most frequent and one hot encode \n",
    "eng_cat_feats = [2, 3, 4]\n",
    "eng_cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "\n",
    "# Random sample imputer on categorical quarter, week and weekofyear_holiday (since difficult to impute precisely) \n",
    "eng_rand_feats = [0, 1, 5] \n",
    "eng_rand_transformer = Pipeline(steps=[\n",
    "    ('imputer', RandomSampleImputer(random_state=0, variables=['0', '1', '2'])),\n",
    "    ('encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# # Categorical variables difficult to impute: quarter, week and weekofyear_holiday\n",
    "# # Impute with a 'missing_value' constant and add missing value indicator\n",
    "# eng_catmiss_feats = [0, 2]\n",
    "# eng_catmiss_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value=\"missing_value\", add_indicator=True)),\n",
    "#     ('encoder', dpOH(categorical_columns=[\"0\",\"1\"]))\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pre-processor objects\n",
    "\n",
    "basic_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('bnum', basic_num_transformer, basic_num_feats),\n",
    "        ('year', year_transformer, year_feat),\n",
    "        ('bmulti', basic_multivar_transformer,basic_multivar_feats )\n",
    "    ])\n",
    "\n",
    "eng_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ecat', eng_cat_transformer, eng_cat_feats),\n",
    "        ('year', year_transformer, year_feat),\n",
    "        ('rand', eng_rand_transformer, eng_rand_feats),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on basic train set...\n",
      "[[3.0 'y1' 34.0 0.0 23.84444444444445 214.9362793 6.315]\n",
      " [2.0 'y3' 19.0 0.0 23.994444444444444 225.2351496 6.664]\n",
      " [1.0 'y3' 2.0 0.0 11.033333333333331 nan 6.832999999999998]\n",
      " [3.0 'y2' 38.0 0.0 17.555555555555557 129.5183333 6.877000000000002]\n",
      " [3.0 'y3' 27.0 0.0 30.48333333333333 130.7196333 7.17]]\n",
      "...Done.\n",
      "[[-2.78693206e-01  8.66209495e-01 -1.11791716e+00  0.00000000e+00\n",
      "   0.00000000e+00  5.67863327e-01  6.64587166e-01  9.33458314e-01\n",
      "   8.74571747e-01]\n",
      " [-2.78693206e-01  8.82265989e-01 -7.39519951e-01  0.00000000e+00\n",
      "   1.00000000e+00 -4.05285581e-01 -4.75946209e-01  1.20681619e+00\n",
      "   8.90608485e-01]\n",
      " [-2.78693206e-01 -5.05133951e-01 -5.56284627e-01  0.00000000e+00\n",
      "   1.00000000e+00 -1.37843449e+00 -1.76855070e+00 -7.26871374e-05\n",
      "  -4.95084497e-01]\n",
      " [-2.78693206e-01  1.93026156e-01 -5.08578389e-01  1.00000000e+00\n",
      "   0.00000000e+00  5.67863327e-01  9.68729399e-01 -1.33374846e+00\n",
      "   2.02216644e-01]\n",
      " [-2.78693206e-01  1.57685799e+00 -1.90898212e-01  0.00000000e+00\n",
      "   1.00000000e+00  5.67863327e-01  1.32338258e-01 -1.30186294e+00\n",
      "   1.58434591e+00]]\n",
      "\n",
      "Performing preprocessings on basic test set...\n",
      "[[nan nan nan 0.0 15.622222222222222 224.1320199 6.832999999999998]\n",
      " [1.0 'y1' 5.0 0.0 7.616666666666668 214.4248812 7.3679999999999986]\n",
      " [nan nan nan 0.0 20.97222222222222 135.4468 8.89]\n",
      " [3.0 'y1' 37.0 0.0 24.066666666666663 214.8785562 6.315]\n",
      " [nan nan nan 0.0 nan 221.4342146 5.943]]\n",
      "...Done.\n",
      "[[-0.27869321 -0.0139242  -0.55628463  1.          0.         -0.00464995\n",
      "  -0.02491361  1.17753636 -0.00447909]\n",
      " [-0.27869321 -0.87086518  0.02377986  0.          0.         -1.37843449\n",
      "  -1.54044402  0.91988452 -0.86036576]\n",
      " [-0.27869321  0.55875739  1.67398201  1.          0.          0.13380974\n",
      "   0.13885343 -1.17639207  0.56749791]\n",
      " [-0.27869321  0.88999689 -1.11791716  0.          0.          0.56786333\n",
      "   0.89269384  0.9319262   0.89832988]\n",
      " [-0.27869321  0.08538818 -1.52125172  1.          0.          0.01490787\n",
      "  -0.00645803  1.10592983  0.08319966]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on basic feats train set\n",
    "print(\"Performing preprocessings on basic train set...\")\n",
    "print(X_basic_train[0:5,:])\n",
    "X_basic_train = basic_preprocessor.fit_transform(X_basic_train)\n",
    "print('...Done.')\n",
    "print(X_basic_train[0:5,:])\n",
    "print()\n",
    "\n",
    "# Preprocessings on basic feats test set\n",
    "print(\"Performing preprocessings on basic test set...\")\n",
    "print(X_basic_test[0:5,:])\n",
    "X_basic_test = basic_preprocessor.transform(X_basic_test)\n",
    "print('...Done.')\n",
    "print(X_basic_test[0:5,:])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing preprocessings on eng train set...\n",
      "[['q3' 'y1' 'mean_temp' 'lowsales_highCPI' 'lowsales_lowunemp' 0.0]\n",
      " ['q2' 'y3' 'mean_temp' 'lowsales_highCPI' 'lowsales_lowunemp' 0.0]\n",
      " ['q1' 'y3' 'mean_temp' nan 'lowsales_lowunemp' 0.0]\n",
      " ['q3' 'y2' 'mean_temp' 'highsales_lowCPI' 'highsales_lowunemp' 0.0]\n",
      " ['q3' 'y3' 'high_temp' 'highsales_lowCPI' 'highsales_lowunemp' 1.0]]\n",
      "...Done.\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 8)\t1.0\n",
      "  (1, 11)\t1.0\n",
      "  (1, 12)\t1.0\n",
      "  (1, 16)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "  (2, 8)\t1.0\n",
      "  (2, 11)\t1.0\n",
      "  (2, 16)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  (3, 6)\t1.0\n",
      "  (3, 10)\t1.0\n",
      "  (3, 13)\t1.0\n",
      "  (3, 15)\t1.0\n",
      "  (4, 2)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "  (4, 11)\t1.0\n",
      "  (4, 13)\t1.0\n",
      "  (4, 16)\t1.0\n",
      "  (4, 17)\t1.0\n",
      "\n",
      "Performing preprocessings on eng test set...\n",
      "[[nan nan 'mean_temp' 'lowsales_highCPI' 'lowsales_lowunemp' 0.0]\n",
      " ['q1' 'y1' 'mean_temp' 'lowsales_highCPI' 'lowsales_lowunemp' 1.0]\n",
      " [nan nan 'mean_temp' 'lowsales_lowCPI' 'lowsales_highunemp' 0.0]\n",
      " ['q3' 'y1' 'mean_temp' 'lowsales_highCPI' 'lowsales_lowunemp' 0.0]\n",
      " [nan nan nan 'lowsales_highCPI' 'lowsales_lowunemp' 0.0]]\n",
      "...Done.\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 10)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 8)\t1.0\n",
      "  (1, 17)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (2, 7)\t1.0\n",
      "  (2, 10)\t1.0\n",
      "  (2, 12)\t1.0\n",
      "  (2, 15)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 8)\t1.0\n",
      "  (3, 13)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (4, 3)\t1.0\n",
      "  (4, 8)\t1.0\n",
      "  (4, 10)\t1.0\n",
      "  (4, 14)\t1.0\n",
      "  (4, 15)\t1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessings on engineered feats train set\n",
    "print(\"Performing preprocessings on eng train set...\")\n",
    "print(X_eng_train[0:5,:])\n",
    "X_eng_train = eng_preprocessor.fit_transform(X_eng_train)\n",
    "print('...Done.')\n",
    "print(X_eng_train[0:5,:])\n",
    "print()\n",
    "\n",
    "# Preprocessings on engineered feats test set\n",
    "print(\"Performing preprocessings on eng test set...\")\n",
    "print(X_eng_test[0:5,:])\n",
    "X_eng_test = eng_preprocessor.transform(X_eng_test)\n",
    "print('...Done.')\n",
    "print(X_eng_test[0:5,:])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac0fa52fb04ce33a0b1a6fe7dce9738c292bebc8dc4d196073405f436c00a9a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('jedha_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
